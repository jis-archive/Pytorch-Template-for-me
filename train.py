import argparse
import collections
import torch
import numpy as np
import os
import pandas as pd

from transformers import (
    AutoModelForSequenceClassification,
    AutoTokenizer,
    DataCollatorWithPadding,
    set_seed,
    Trainer,
    TrainingArguments,
    AddedToken
)

import src.metrics as module_metric
from src.dataset import ReviewDataset
from src.config import ConfigParser
from src.preprocessing import load_and_preprocess, load_and_preprocess_test

def train_model(config, ensemble_model_path=None):
    logger = config.get_logger('train')

    model_path = ensemble_model_path if ensemble_model_path else config['arch']['args']['model_path']
    tokenizer = AutoTokenizer.from_pretrained(model_path)

    model = AutoModelForSequenceClassification.from_pretrained(
        model_path, 
        num_labels=config['arch']['args'].get('num_labels', 2)    
    )

    new_domain_tokens =[
        "ã… ã… ", "ã…œã…œ",
        "ã…‰ã…‰", "ã…¡ã…¡", "-_-",
    ]

    added = [AddedToken(t, single_word=True, lstrip=False, rstrip=False, normalized=False) for t in new_domain_tokens]
    num_added = tokenizer.add_tokens(added)
    print(f"Added {num_added} new tokens to vocab. (total size: {len(tokenizer)})")
                           
    if num_added > 0:
        model.resize_token_embeddings(len(tokenizer))
        print(f"âœ… Model embedding resized to {len(tokenizer)}") 


    compute_metrics = getattr(module_metric, 'compute_metrics')
    logger.info(model)

    train_data, val_data = load_and_preprocess('./data/train.csv')

    train_dataset = ReviewDataset(train_data["review"], train_data["label"], tokenizer, config['data_loader']['args']['max_length'])
    eval_dataset = ReviewDataset(val_data["review"], val_data["label"], tokenizer, config['data_loader']['args']['max_length'])

    training_args = TrainingArguments(
        output_dir=config['trainer']['save_dir'],
        num_train_epochs=config['trainer']['epochs'],

        per_device_train_batch_size=config['data_loader']['args']['train_batch_size'],
        per_device_eval_batch_size=config['data_loader']['args']['eval_batch_size'],

        warmup_steps=config['trainer']['args'].get('warmup_steps', 0),
        weight_decay=config['trainer']['args'].get('weight_decay', 0.0),
        learning_rate=config['optimizer']['args'].get('lr', 5e-5),

        logging_strategy=config['trainer']['args'].get('logging_strategy', 'epoch'),
        logging_steps=config['trainer']['args'].get('logging_steps', 100),
        eval_strategy=config['trainer']['args'].get('eval_strategy', 'epoch'),

        save_strategy="epoch" if config['trainer'].get('is_save_model', True) else "no",
        load_best_model_at_end=config['trainer'].get('is_save_model', True),
        metric_for_best_model="accuracy" if config['trainer'].get('is_save_model', True) else None,
        greater_is_better=True,
        save_total_limit=2 if config['trainer']['is_save_model'] else 0,

        report_to="wandb" if config['trainer']['args'].get('use_wandb', False) else "none",
        run_name=config['trainer']['args'].get('run_name', "hf-train-run"),

        seed=config['trainer']['args'].get('random_state', 42),
        fp16=torch.cuda.is_available(),
        dataloader_num_workers=config['data_loader']['args'].get('num_workers', 2),
        remove_unused_columns=config['trainer']['args'].get('remove_unused_columns', False),
        push_to_hub=config['trainer']['args'].get('push_to_hub', False),
        gradient_accumulation_steps=config['trainer']['args'].get('gradient_accumulation_steps', 1),
        logging_first_step=config['trainer']['args'].get('logging_first_step', True)
    )

    trainer=Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=eval_dataset,
        data_collator=DataCollatorWithPadding(tokenizer=tokenizer),
        compute_metrics=compute_metrics
    )

    # í›ˆë ¨ ì •ë³´ ì¶œë ¥
    print(f"í›ˆë ¨ ìƒ˜í”Œ: {len(train_dataset):,}ê°œ")
    print(f"ê²€ì¦ ìƒ˜í”Œ: {len(eval_dataset):,}ê°œ")
    print(f"í›ˆë ¨ ì—í¬í¬: {training_args.num_train_epochs}íšŒ")
    print(f"ë°°ì¹˜ í¬ê¸°: {config['data_loader']['args']['train_batch_size']} (í›ˆë ¨) / {config['data_loader']['args']['eval_batch_size']} (ê²€ì¦)")
    print(f"í•™ìŠµë¥ : {config['optimizer']['args'].get('lr', 5e-5)}")
    print(f"ì‹œë“œê°’: {config['trainer']['args'].get('random_state', 42)}")

    print(f"wandb ì‚¬ìš©: {config['trainer']['args'].get('use_wandb', False)}")

    # í›ˆë ¨ ì‹¤í–‰
    try:
        training_results = trainer.train()
        print("\ní›ˆë ¨ ì™„ë£Œ")
        print(f"ìµœì¢… í›ˆë ¨ ì†ì‹¤: {training_results.training_loss:.4f}")
        eval_results = trainer.evaluate()
    
        # ê²°ê³¼ ì¶”ì¶œ
        accuracy = eval_results.get('eval_accuracy', 0.0)

        # í›ˆë ¨ ë¡œê·¸ ì •ë³´ ì¶œë ¥
        if hasattr(training_results, "log_history"):
            print(f"ì´ í›ˆë ¨ ìŠ¤í…: {training_results.global_step}")

    except KeyboardInterrupt:
        print("\nì‚¬ìš©ìì— ì˜í•´ í›ˆë ¨ì´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
        raise
    except Exception as e:
        print(f"\ní›ˆë ¨ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        raise

    return model, tokenizer, accuracy
    

def ensemble_train(config):
    logger = config.get_logger('ensemble')
    ensemble_paths = config['ensemble']['model_paths']

    trained_models = []
    tokenizers = []
    model_accuracies = []
    for path in ensemble_paths:
        tmp_model, tmp_tokenizer, tmp_accuracy = train_model(config, ensemble_model_path=path)

        trained_models.append(tmp_model)
        tokenizers.append(tmp_tokenizer)
        model_accuracies.append(tmp_accuracy)

        torch.cuda.empty_cache()
    return trained_models, tokenizers, model_accuracies

def setup_device(args):
    """
    GPU ì„¤ì • ë° ì •ë³´ ì¶œë ¥
    """
    # 1ï¸âƒ£ ì„ íƒëœ GPU ì¸ë±ìŠ¤ í™˜ê²½ë³€ìˆ˜ì— ë°˜ì˜
    if args.device:
        os.environ["CUDA_VISIBLE_DEVICES"] = args.device
        print(f"âœ… ì§€ì •ëœ GPU: {args.device}")
    else:
        print("âš ï¸ GPU ì¸ë±ìŠ¤ê°€ ì§€ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ëª¨ë“  GPU ë˜ëŠ” CPUë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")

    # 2ï¸âƒ£ PyTorch ë””ë°”ì´ìŠ¤ ì„¤ì •
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    # 3ï¸âƒ£ GPU ìƒíƒœ ì ê²€
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        gpu_count = torch.cuda.device_count()
        print(f"âœ… GPU {gpu_count}ê°œ ì‚¬ìš© ê°€ëŠ¥: {device}")
        for i in range(gpu_count):
            print(f"   GPU {i}: {torch.cuda.get_device_name(i)}")
    else:
        print("âš ï¸ CUDA ì‚¬ìš© ë¶ˆê°€ - CPUë¡œ í›ˆë ¨ ì§„í–‰")

    return device


def inference_ensemble(config, trained_models, tokenizers, model_accuracies):
    # ========================================
    # ğŸ¤– ì•™ìƒë¸” ëª¨ë¸ Setup (Weighted Soft Voting - Exponential)
    # ========================================
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    LABEL_MAPPING = {0: "ê°•í•œ ë¶€ì •", 1: "ì•½í•œ ë¶€ì •", 2: "ì•½í•œ ê¸ì •", 3: "ê°•í•œ ê¸ì •"}
    model_names = [
        "klue/roberta-base", 
        "klue/bert-base", 
        "kykim/bert-kor-base", 
        "beomi/kcbert-base", 
        "monologg/koelectra-base-v3-discriminator",
    ]   

    print(model_accuracies)
    df_test = pd.read_csv("./data/test.csv")

    print(f"\n{'='*60}")
    print(f"ğŸ¯ ì•™ìƒë¸” ì¶”ë¡  ì¤€ë¹„ (Weighted Soft Voting - Exponential)")
    print(f"{'='*60}")
    print(f"ì•™ìƒë¸” ëª¨ë¸ ìˆ˜: {len(trained_models)}ê°œ")
        

   
    # Exponential ë°©ë²•ìœ¼ë¡œ ê°€ì¤‘ì¹˜ ê³„ì‚°

    print(f"\n{'='*60}")
    print("ğŸ¯ Exponential ë°©ë²•ìœ¼ë¡œ ê°€ì¤‘ì¹˜ ê³„ì‚°")
    print(f"{'='*60}")

    model_accuracies = np.array(model_accuracies)

    # Exponential ê°€ì¤‘ì¹˜ ê³„ì‚°
    EXP_SCALE = 30  # ìŠ¤ì¼€ì¼ íŒŒë¼ë¯¸í„°
    model_weights = np.exp(model_accuracies * EXP_SCALE) / np.exp(model_accuracies * EXP_SCALE).sum()

    print(f"\nâš™ï¸ Exponential Scale: {EXP_SCALE}")
    print(f"\nê° ëª¨ë¸ì˜ ê°€ì¤‘ì¹˜:")
    print("-" * 80)
    for idx, (name, acc, weight) in enumerate(zip(model_names, model_accuracies, model_weights)):
        model_name = name.split('/')[-1]
        print(f"  [{idx+1}] {model_name:30s} | Accuracy: {acc:.4f} | Weight: {weight:.4f} ({weight*100:.1f}%)")
    print("-" * 80)

    # ê°€ì¤‘ì¹˜ í†µê³„
    print(f"\nğŸ“Š ê°€ì¤‘ì¹˜ í†µê³„:")
    print(f"   í•©ê³„: {model_weights.sum():.4f}")
    print(f"   ìµœëŒ€: {model_weights.max():.4f} ({model_weights.max()*100:.1f}%)")
    print(f"   ìµœì†Œ: {model_weights.min():.4f} ({model_weights.min()*100:.1f}%)")
    print(f"   ìµœëŒ€/ìµœì†Œ ë¹„ìœ¨: {model_weights.max() / model_weights.min():.2f}ë°°")
    print(f"   í‘œì¤€í¸ì°¨: {model_weights.std():.4f}")


    # í…ŒìŠ¤íŠ¸ ë°ì´í„° ì¤€ë¹„

    print(f"\n{'='*60}")
    print("ğŸ“‹ í…ŒìŠ¤íŠ¸ ë°ì´í„° ì¤€ë¹„")
    print(f"{'='*60}")

    # í…ŒìŠ¤íŠ¸ ë°ì´í„° ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì ìš©
    print("\ní…ŒìŠ¤íŠ¸ ë°ì´í„°ì— ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì ìš©...")
    test_texts = df_test["review"].tolist()
    test_processed = load_and_preprocess_test(test_texts)

    # ì „ì²˜ë¦¬ëœ í…ŒìŠ¤íŠ¸ ë°ì´í„° ì¤€ë¹„
    test_data = pd.DataFrame(
        {
            "ID": df_test["ID"],
            "review": test_processed,
            "label": [-1] * len(df_test),  # í…ŒìŠ¤íŠ¸ ë°ì´í„°ëŠ” ë ˆì´ë¸” ì—†ìŒ (ë”ë¯¸ ê°’)
        }
    ).reset_index(drop=True)

    print(f"í…ŒìŠ¤íŠ¸ ë°ì´í„°: {len(test_data):,} ìƒ˜í”Œ")

    
    # ê°€ì¤‘ ì•™ìƒë¸” ì¶”ë¡  ìˆ˜í–‰ (Exponential ê°€ì¤‘ì¹˜)

    print(f"\n{'='*60}")
    print("ğŸ”® ê°€ì¤‘ ì•™ìƒë¸” ì¶”ë¡  ì‹œì‘ (Exponential ê°€ì¤‘ì¹˜ ì ìš©)...")
    print(f"{'='*60}")

    all_model_probs = []  # ê° ëª¨ë¸ì˜ ê°€ì¤‘ í™•ë¥ ê°’ ì €ì¥

    for idx, (model, tokenizer, weight) in enumerate(zip(trained_models, tokenizers, model_weights)):
        model_name = model_names[idx].split('/')[-1]
        print(f"\n[{idx+1}/{len(trained_models)}] ğŸ¤– {model_name} ì˜ˆì¸¡ ì¤‘... (weight: {weight:.4f})")
        
        # ëª¨ë¸ì„ GPUë¡œ ì´ë™ ë° eval ëª¨ë“œ
        model.eval()
        model = model.to(device)
        
        # í•´ë‹¹ ëª¨ë¸ì˜ í† í¬ë‚˜ì´ì €ë¡œ ë°ì´í„°ì…‹ ìƒì„±
        test_dataset = ReviewDataset(
            test_data["review"], None, tokenizer, config['data_loader']['args']['max_length']
        )
        
        # DataLoader ìƒì„±
        test_dataloader = torch.utils.data.DataLoader(
            test_dataset,
            batch_size=config['data_loader']['args']['eval_batch_size'],
            shuffle=False,
            collate_fn=DataCollatorWithPadding(tokenizer=tokenizer)
        )
        
        # ë°°ì¹˜ë³„ ì˜ˆì¸¡ ìˆ˜í–‰
        model_probs = []
        with torch.no_grad():
            for batch in test_dataloader:
                # ë°°ì¹˜ë¥¼ ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
                batch = {k: v.to(device) for k, v in batch.items()}
                
                # ì˜ˆì¸¡
                outputs = model(**batch)
                probs = torch.softmax(outputs.logits, dim=-1)  # í™•ë¥ ë¡œ ë³€í™˜
                
                # *** Exponential ê°€ì¤‘ì¹˜ ì ìš© ***
                weighted_probs = probs.cpu().numpy() * weight
                model_probs.append(weighted_probs)
        
        # ëª¨ë“  ë°°ì¹˜ ê²°ê³¼ í•©ì¹˜ê¸°
        model_probs = np.vstack(model_probs)
        all_model_probs.append(model_probs)
        
        print(f"   âœ… {model_name} ì˜ˆì¸¡ ì™„ë£Œ (shape: {model_probs.shape})")
        
        # GPU ë©”ëª¨ë¦¬ í•´ì œ
        model = model.cpu()
        torch.cuda.empty_cache()

    # ========================================
    # ğŸ¯ Weighted Soft Voting: ê°€ì¤‘ì¹˜ ì ìš©ëœ í™•ë¥  í•©ì‚°
    # ========================================

    print(f"\n{'='*60}")
    print("ğŸ“Š Weighted Soft Voting ê³„ì‚° ì¤‘...")
    print(f"{'='*60}")

    # ê° ëª¨ë¸ì˜ ê°€ì¤‘ì¹˜ê°€ ì ìš©ëœ í™•ë¥ ì„ í•©ì‚°
    ensemble_probs = np.sum(all_model_probs, axis=0)  # shape: (ìƒ˜í”Œ ìˆ˜, í´ë˜ìŠ¤ ìˆ˜)
    predicted_labels = np.argmax(ensemble_probs, axis=1)

    print(f"ì¶”ë¡  ì™„ë£Œ: {len(predicted_labels):,}ê°œ ì˜ˆì¸¡")

    # ì›ë³¸ df_testì— pred ì»¬ëŸ¼ ì¶”ê°€
    df_test["pred"] = predicted_labels

    print(f"\ndf_testì— pred ì»¬ëŸ¼ì´ ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤. í˜•íƒœ: {df_test.shape}")

    # ========================================
    # ğŸ“ˆ ê²°ê³¼ ë¶„ì„
    # ========================================

    print(f"\n{'='*60}")
    print("ğŸ“ˆ ì˜ˆì¸¡ ê²°ê³¼ ë¶„ì„")
    print(f"{'='*60}")

    unique_predictions, counts = np.unique(predicted_labels, return_counts=True)
    print("\ní´ë˜ìŠ¤ë³„ ì˜ˆì¸¡ ë¶„í¬:")
    for pred, count in zip(unique_predictions, counts):
        percentage = (count / len(predicted_labels)) * 100
        class_name = LABEL_MAPPING.get(pred, f"í´ë˜ìŠ¤ {pred}")
        print(f"   {class_name} ({pred}): {count:,}ê°œ ({percentage:.1f}%)")

    # ì˜ˆì¸¡ í™•ì‹ ë„ ë¶„ì„
    confidence_scores = np.max(ensemble_probs, axis=1)
    print(f"\nğŸ“Š ì•™ìƒë¸” ì˜ˆì¸¡ í™•ì‹ ë„ í†µê³„:")
    print(f"   í‰ê·  í™•ì‹ ë„: {confidence_scores.mean():.4f}")
    print(f"   ìµœì†Œ í™•ì‹ ë„: {confidence_scores.min():.4f}")
    print(f"   ìµœëŒ€ í™•ì‹ ë„: {confidence_scores.max():.4f}")
    print(f"   ì¤‘ê°„ê°’: {np.median(confidence_scores):.4f}")

    # ë‚®ì€ í™•ì‹ ë„ ìƒ˜í”Œ í™•ì¸
    low_confidence_threshold = 0.4
    low_confidence_count = np.sum(confidence_scores < low_confidence_threshold)
    print(f"   í™•ì‹ ë„ < {low_confidence_threshold}: {low_confidence_count}ê°œ ({low_confidence_count/len(confidence_scores)*100:.1f}%)")

    # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
    if torch.cuda.is_available():
        torch.cuda.empty_cache()

    print(f"\n{'='*60}")
    print("âœ… Exponential ê°€ì¤‘ ì•™ìƒë¸” ì¶”ë¡  ì™„ë£Œ!")
    print(f"{'='*60}")

    # ========================================
    # ğŸ’¾ ì œì¶œ íŒŒì¼ ìƒì„±
    # ========================================

    print(f"\n{'='*60}")
    print("ğŸ’¾ ì œì¶œ íŒŒì¼ ìƒì„± ì¤‘...")
    print(f"{'='*60}")




def main():
    parser = argparse.ArgumentParser(description='Train Script with Ensemble Option')
    parser.add_argument('-c', '--config', default='./configs/config.json', type=str, help='config file path')
    parser.add_argument('-m', '--mode', default='single', choices=['single', 'ensemble', 's', 'e'], help='training mode')
    parser.add_argument('-r', '--resume', default=None, type=str, help='path to checkpoint')
    parser.add_argument('-d', '--device', default=None, type=str, help='GPU indices to enable')
    args = parser.parse_args()


    device = setup_device(args)

    # CLIë¡œ lr, batch size ì¡°ì •
    CustomArgs = collections.namedtuple('CustomArgs', 'flags type target')
    options = [
        CustomArgs(['--lr', '--learning_rate'], type=float, target='optimizer;args;lr'),
        CustomArgs(['--bs', '--batch_size'], type=int, target='data_loader;args;batch_size')
    ]

    config = ConfigParser.from_args(parser, options)

    seed = config['trainer']['args'].get('random_state', 42)
    set_seed(seed)

    mode = config.args.mode.lower()

    if mode in ['ensemble', 'e']:
        trained_models, tokenizers, model_accuracies = ensemble_train(config)
        inference_ensemble(config, trained_models, tokenizers, model_accuracies)
    else:
        train_model(config)



if __name__ == '__main__':
    main()
